{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONaUAdc/r6h9xnFIIWoX8t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ABDULRAFAY757/SYNC-INTERN-S/blob/main/chatBOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqLyckkTodo4",
        "outputId": "cf3f3309-f796-43fb-f665-e2cf8dfcb3ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip -q install transformers "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from transformers import TFAutoModelForCausalLM, AutoTokenizer,AutoModelForCausalLM\n",
        "import tensorflow as tf\n",
        "from transformers.utils import logging\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "XtxLYAstopro"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatBot():\n",
        "    \"\"\"\n",
        "    A class representing a chatbot that uses the DialoGPT-medium model to generate responses.\n",
        "\n",
        "    Attributes:\n",
        "        chat_history_ids (tf.Tensor): Tensor containing the chat history of the conversation.\n",
        "        user_input_ids (tf.Tensor): Tensor containing the user's input.\n",
        "        end_chat (bool): Flag indicating if the chat should be ended.\n",
        "        tokenizer (AutoTokenizer): Tokenizer for encoding and decoding text.\n",
        "        model (TFAutoModelForCausalLM): DialoGPT-medium model for generating responses.\n",
        "\n",
        "    Methods:\n",
        "        welcome(): Greets the user and provides instructions on how to end the chat.\n",
        "        get_user_input(): Gets user input and sets the user_input_ids attribute.\n",
        "        get_bot_response(): Generates a response from the chatbot and prints it.\n",
        "        get_random_response(): Generates a random response for the chatbot.\n",
        "        start_chatting(): Starts the chatbot conversation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the ChatBot instance.\n",
        "\n",
        "        Sets the chat_history_ids and user_input_ids attributes to None, and the end_chat attribute to False.\n",
        "        Initializes the tokenizer and model attributes with the DialoGPT-medium model.\n",
        "        Calls the welcome method to greet the user.\n",
        "\n",
        "        Parameters:\n",
        "        None\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        # Set the initial chat history and user input IDs to None, and end_chat flag to False\n",
        "        self.chat_history_ids = None\n",
        "        self.user_input_ids = None\n",
        "        self.end_chat = False\n",
        "\n",
        "        # Initialize the tokenizer and model with the DialoGPT-medium model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
        "        self.model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "        # Call the welcome method to greet the user\n",
        "        self.welcome()\n",
        "\n",
        "\n",
        "    def welcome(self):\n",
        "        \"\"\"\n",
        "        Prints a greeting message, informs the user how to end the chat, \n",
        "        and provides instructions on how to interact with the ChatBot.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        print(\"Initializing ChatBot ...\")\n",
        "        time.sleep(2)\n",
        "        print('Type \"bye\" or \"quit\" or \"exit\" to end chat \\n')\n",
        "        time.sleep(3)\n",
        "        greeting = np.random.choice([\n",
        "            \"Welcome, I am ChatBot, here for your kind service\",\n",
        "            \"Hey, Great day! I am your virtual assistant\",\n",
        "            \"Hello, it's my pleasure meeting you\",\n",
        "        ])\n",
        "        print(\"ChatBot >>  \" + greeting)\n",
        "\n",
        "\n",
        "    def get_user_input(self):\n",
        "        \"\"\"\n",
        "        Gets user input and sets the user_input_ids attribute.\n",
        "\n",
        "        If the user types \"bye\", \"quit\", or \"exit\", sets the end_chat attribute to True.\n",
        "        \"\"\"\n",
        "        text = input(\"User    >> \")\n",
        "        if text.lower().strip() in ['bye', 'quit', 'exit']:\n",
        "            self.end_chat = True\n",
        "            print('ChatBot >>  See you soon! Bye!')\n",
        "\n",
        "            print('\\nQuitting ChatBot ...')\n",
        "        else:\n",
        "            self.user_input_ids = self.tokenizer.encode(text + self.tokenizer.eos_token, \\\n",
        "                                                        return_tensors='tf')\n",
        "    \n",
        "    def get_bot_response(self):\n",
        "        \"\"\"\n",
        "        Generates a response from the chatbot and prints it.\n",
        "\n",
        "        If there is chat history, concatenates it with the user input to generate the bot_input_ids.\n",
        "        Otherwise, uses the user input ids as the bot_input_ids.\n",
        "        Generates a response from the model using the bot_input_ids.\n",
        "        Decodes the response using the tokenizer and sets it to the response variable.\n",
        "        If the response is empty, calls the get_random_response method to generate a random response.\n",
        "        Prints the response.\n",
        "        Resets chat_history_ids to None after each response is generated.\n",
        "        \"\"\"\n",
        "        if self.chat_history_ids is not None:\n",
        "            # If there is chat history, concatenate it with the user input to generate bot_input_ids\n",
        "            bot_input_ids = tf.concat([self.chat_history_ids, self.user_input_ids], axis=-1)\n",
        "        else:\n",
        "            # Otherwise, use the user input ids as the bot_input_ids\n",
        "            bot_input_ids = self.user_input_ids\n",
        "\n",
        "        # Generate a response from the model using the bot_input_ids\n",
        "        self.chat_history_ids = self.model.generate(bot_input_ids, max_length=1000, \\\n",
        "                                                    pad_token_id=self.tokenizer.eos_token_id)\n",
        "\n",
        "        # Decode the response using the tokenizer and set it to the response variable\n",
        "        response = self.tokenizer.decode(self.chat_history_ids[:, self.user_input_ids.shape[-1]:][0], \\\n",
        "                                         skip_special_tokens=True)\n",
        "\n",
        "        # If the response is empty, call the get_random_response method to generate a random response\n",
        "        if response == \"\":\n",
        "            response = self.get_random_response()\n",
        "\n",
        "        # Print the response\n",
        "        print('ChatBot >>  ' + response)\n",
        "\n",
        "        # Reset chat_history_ids after each response is generated\n",
        "        self.chat_history_ids = None\n",
        "\n",
        "\n",
        "    def get_random_response(self):\n",
        "        \"\"\"\n",
        "        Returns a random response when the bot is unable to generate a valid response.\n",
        "\n",
        "        If the bot is unable to generate a response, this method searches through the chat history to find the most recent \n",
        "        non-empty response. It does this by iterating backwards through the chat history, starting with the most recent \n",
        "        message and ending with the earliest message. If the non-empty response ends with a question mark, the method \n",
        "        returns a random reply from the list of \"I don't know\" and \"I am not sure\". Otherwise, the method returns a \n",
        "        random reply from the list of \"Great\", \"Fine. What's up?\", and \"Okay\".\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        reply : str\n",
        "            A random response string.\n",
        "        \"\"\"\n",
        "        # Start by checking the most recent response in the chat history\n",
        "        i = -1\n",
        "        response = self.tokenizer.decode(self.chat_history_ids[:, self.user_input_ids.shape[i]:][0], skip_special_tokens=True)\n",
        "        \n",
        "        # If the most recent response is empty, search backwards through chat history to find the most recent non-empty response\n",
        "        while response == '':\n",
        "            i = i - 1\n",
        "            response = self.tokenizer.decode(self.chat_history_ids[:, self.user_input_ids.shape[i]:][0], skip_special_tokens=True)\n",
        "        \n",
        "        # If the non-empty response ends with a question mark, return a random reply from the list \n",
        "        if response.strip() == '?':\n",
        "            reply = np.random.choice([\"I don't know\", \"I am not sure\"])\n",
        "        # Otherwise, return a random reply from the list \n",
        "        else:\n",
        "            reply = np.random.choice([\"Great\", \"Fine. What's up?\", \"Okay\"])\n",
        "        \n",
        "        return reply\n",
        "\n",
        "    @classmethod\n",
        "    def start_chatting(cls):\n",
        "        \"\"\"\n",
        "        Function to start the chatbot conversation.\n",
        "\n",
        "        The function uses a while loop to repeatedly ask the user for input,\n",
        "        generates a response using the get_bot_response() function, and prints\n",
        "        the response. If the user types \"bye\", \"quit\", or \"exit\", the loop is\n",
        "        terminated and the function returns.\n",
        "\n",
        "        Args:\n",
        "            cls: Class object\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        # create an instance of the class\n",
        "        chatbot = cls()\n",
        "\n",
        "        # Loop to repeatedly ask for user input and generate responses\n",
        "        while True:\n",
        "            # Get input from the user and store it in a variable\n",
        "            user_input = chatbot.get_user_input()\n",
        "\n",
        "            # If user input indicates end of conversation, break the loop\n",
        "            if chatbot.end_chat:\n",
        "                break\n",
        "\n",
        "            # Generate and print a response from the chatbot using the user input\n",
        "            chatbot.get_bot_response()"
      ],
      "metadata": {
        "id": "j-kFT9PEougf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    ChatBot.start_chatting()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whxmRIzdruWL",
        "outputId": "9946330f-d5ed-4fdd-fb19-5290a94fd592"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing ChatBot ...\n",
            "Type \"bye\" or \"quit\" or \"exit\" to end chat \n",
            "\n",
            "ChatBot >>  Welcome, I am ChatBot, here for your kind service\n",
            "User    >> Hello!\n",
            "ChatBot >>  Hello! :D\n",
            "User    >> Teach me python\n",
            "ChatBot >>  I'm not sure if you're serious or not, but I'm sure there's a subreddit for that.\n",
            "User    >> What would you recommmend to an individual who want to start blogging?\n",
            "ChatBot >>  I'm not sure, but I would recommend starting with a blog.\n",
            "User    >> Your take on people who fakes alot?\n",
            "ChatBot >>  I don't think I've ever fakes.\n",
            "User    >> Niceeee,thanks\n",
            "ChatBot >>  No problem, enjoy!\n",
            "User    >> CYA, bye\n",
            "ChatBot >>  I'm sorry, I'm not sure what that means.\n",
            "User    >> bye\n",
            "ChatBot >>  See you soon! Bye!\n",
            "\n",
            "Quitting ChatBot ...\n"
          ]
        }
      ]
    }
  ]
}